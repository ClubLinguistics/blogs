<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://clublinguistics.github.io/blogs/feed.xml" rel="self" type="application/atom+xml" /><link href="https://clublinguistics.github.io/blogs/" rel="alternate" type="text/html" /><updated>2021-07-02T02:40:44-05:00</updated><id>https://clublinguistics.github.io/blogs/feed.xml</id><title type="html">Club Linguistics</title><subtitle>Community to promote various stories from various domains in various languages.</subtitle><entry><title type="html">What Is ROC Curve?</title><link href="https://clublinguistics.github.io/blogs/machine%20learning/2020/12/26/roc.html" rel="alternate" type="text/html" title="What Is ROC Curve?" /><published>2020-12-26T00:00:00-06:00</published><updated>2020-12-26T00:00:00-06:00</updated><id>https://clublinguistics.github.io/blogs/machine%20learning/2020/12/26/roc</id><content type="html" xml:base="https://clublinguistics.github.io/blogs/machine%20learning/2020/12/26/roc.html">&lt;p&gt;Consider &lt;strong&gt;heart data&lt;/strong&gt; which consist of 13 features such as &lt;strong&gt;age, sex, chol (cholesterol measurement)&lt;/strong&gt;. Our goal is to predict whether an individual has heart disease based on the above features, this is a &lt;strong&gt;binary classification problem&lt;/strong&gt;. That is, with only two classes. Let us suppose we have 100 samples (a sample corresponds to patient information) amongst which 90 samples are positive (have a heart disease) so if you say that all patients (100 of them)have heart disease, you have correctly classified 90/100 samples. You didn’t even build the model and got an accuracy of 90%. But, if we look carefully we will see that the dataset is skewed. That is, number of positive samples is far more than the negative samples (patients without heart disease). So, it is not advisable to decide best model just on the basis of accuracy because, it does not represent the data completely. So, you might get high accuracy, but your model will probably not perform that well when it comes to real-world samples. Therefore, we need a more reliable evaluation metric and hence, ROC comes into the picture.&lt;/p&gt;

&lt;p&gt;Here we will use &lt;a href=&quot;https://medium.com/swlh/what-is-svm-f4e06eca79b3&quot;&gt;SVM (support vector machine)&lt;/a&gt; as model with different values of gamma (parameter) for comparison and with the help of ROC curve figure out which value of gamma gives us the optimal result (best model). Let’s see this in detail.&lt;/p&gt;

&lt;p&gt;Rather than predicting samples are positive or not, we predict the probability of having heart disease for each sample, and if this probability is greater than the threshold, we say the given patient has heart disease. So let’s say we select a threshold value of 0.1 therefore, if the probability is greater than 0.1 we say that particular patient has heart disease. Now as we vary the threshold it is obvious that prediction will also vary. For example, if the threshold is 0, we will predict every sample in the data is positive (since, probability is greater than equals to 0). Similarly, if we have a high threshold of say 0.9, &lt;strong&gt;even though the patient has heart disease it is possible that prediction will be ‘no heart disease’ which is very risky!&lt;/strong&gt; So we have to choose a threshold in such a way that not only we must have good predictions but also balance the tradeoff. Now we give a summary. Table.1 shows the possible results when applying a classiﬁer to the data, this table is also known as &lt;strong&gt;confusion matrix&lt;/strong&gt;. Now, let us define terms given in the table require to build ROC curve.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/table_1_to_upload.png&quot; alt=&quot;&quot; title=&quot;Source: 'Understanding Confusion matrix and applying it on KNN-Classifier on Iris Data set' by Vishwanath Beena on medium&quot; /&gt;&lt;/p&gt;

&lt;p&gt;True positive (TP): Given a patient’s information (age, sex… values), if your model predicts heart disease, and the patient actually has heart disease then, it is considered a true positive.&lt;/p&gt;

&lt;p&gt;True negative (TN): Given a patient’s information, if your model predicts no heart disease, and the patient actually has no heart disease then, it is considered a true negative.&lt;/p&gt;

&lt;p&gt;In simple words, if your model correctly predicts positive class, it is true positive, and if your model correctly predicts negative class, it is a true negative.&lt;/p&gt;

&lt;p&gt;False positive (FP): Given a patient’s information, if your model predicts heart disease, and the patient actually has no heart disease then, it is considered a false positive.&lt;/p&gt;

&lt;p&gt;False negative(FN): Given a patient’s information, if your model predicts no heart disease, and the patient actually has heart disease then, it is considered a false negative.&lt;/p&gt;

&lt;p&gt;In simple words, if your model incorrectly (or falsely) predicts positive class, it is a false positive. If your model incorrectly (or falsely) predicts negative class, it is a false negative. Now,&lt;/p&gt;

&lt;p&gt;TPR/Sensitivity is defined as:&lt;/p&gt;

&lt;p&gt;TP / P&lt;/p&gt;

&lt;p&gt;Where P = TP+FN&lt;/p&gt;

&lt;p&gt;TPR is ‘The fraction of patients with heart disease which are correctly identified’.&lt;/p&gt;

&lt;p&gt;FPR/False Positive Rate is defined as:&lt;/p&gt;

&lt;p&gt;FP / N&lt;/p&gt;

&lt;p&gt;FPR is ‘The fraction of patients without heart disease which are incorrectly identified as with heart disease’.&lt;/p&gt;

&lt;p&gt;Specificity is defined as:&lt;/p&gt;

&lt;p&gt;TN / N&lt;/p&gt;

&lt;p&gt;Where N = TN+FP&lt;/p&gt;

&lt;p&gt;Specificity is ‘The fraction of patients without heart disease which are correctly identified’.&lt;/p&gt;

&lt;p&gt;ROC provides a simple way to summarize the information related to different thresholds and resulting TPR and FPR values.Table.2 illustrates TPR and FPR values for different thresholds.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/table_2_to_upload.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You might wonder why some threshold values are negative? I will explain this later. Now, Rather than building different tables (like Table.1.) for different values of threshold, you can just look at ROC curve to decide what threshold to select. Most of the time, the top-left value on ROC curve should give you a quite good threshold, as illustrated in Fig.1, and the corresponding threshold value is highlighted in Table.2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/Fig_1_to_upload.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Depend on how many false positive you are willing to accept, you decide optimal threshold. For example, if you don’t want to have too many false positives you should have a high threshold value. This will however, also give you a lot more false negatives. Observe the trade-off and select the best threshold, &lt;strong&gt;by decreasing the threshold value of TPR, FPR increases and specificity decreases&lt;/strong&gt;. Similarly, when we increase the threshold TPR, FPR decreases but specificity increases.&lt;/p&gt;

&lt;p&gt;AUC makes it easy to compare one ROC curve to another, &lt;strong&gt;larger the area under the curve the better the model.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/Fig_2_to_upload.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In Fig.2.The AUC for SVM with gamma is equals to 0.001is 0.88, the AUC for SVM with gamma is equals to 0.0001 is 0.76, and the AUC for SVM with gamma is equals to 0.00001 is 0.75. We say SVM with gamma is equals to 0.001 is better model than others, since, 0.88 is close to maximum value of AUC that is one, AUC corresponds to SVM with gamma is equals to 0.001 is illustrated in Fig.1., we expect a classifier that performs no better than chance to have an AUC of 0.5, the no information classifier in Fig.2 (red line) predicts every patient as ’with heart disease’ independent of actual target (class).&lt;/p&gt;

&lt;p&gt;You can also compare other classification models like: Logistic Regression, KNN classifier with SVM.&lt;/p&gt;

&lt;p&gt;AUC is a widely used metric for binary classification tasks in the industry, and a metric everyone should know about.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: In general we use probabilities for comparison with the threshold. In case of SVM, decision function values are computed and are compared with the threshold, and can take positive or negative values, which can be seen in Table.2. But the prediction is done in the same way: if function value greater than threshold we assign the sample to the positive class (patient predicted as, ‘with disease’), otherwise we assign the sample to negative class (patient predicted as, ‘without disease’).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Approaching (Almost) Any Machine Learning Problem, book by Abhishek Thakur.&lt;/li&gt;
  &lt;li&gt;An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie, Daniela Witten.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Saurav Jadhav</name></author><category term="machine learning" /><summary type="html">Consider heart data which consist of 13 features such as age, sex, chol (cholesterol measurement). Our goal is to predict whether an individual has heart disease based on the above features, this is a binary classification problem. That is, with only two classes. Let us suppose we have 100 samples (a sample corresponds to patient information) amongst which 90 samples are positive (have a heart disease) so if you say that all patients (100 of them)have heart disease, you have correctly classified 90/100 samples. You didn’t even build the model and got an accuracy of 90%. But, if we look carefully we will see that the dataset is skewed. That is, number of positive samples is far more than the negative samples (patients without heart disease). So, it is not advisable to decide best model just on the basis of accuracy because, it does not represent the data completely. So, you might get high accuracy, but your model will probably not perform that well when it comes to real-world samples. Therefore, we need a more reliable evaluation metric and hence, ROC comes into the picture. Here we will use SVM (support vector machine) as model with different values of gamma (parameter) for comparison and with the help of ROC curve figure out which value of gamma gives us the optimal result (best model). Let’s see this in detail. Rather than predicting samples are positive or not, we predict the probability of having heart disease for each sample, and if this probability is greater than the threshold, we say the given patient has heart disease. So let’s say we select a threshold value of 0.1 therefore, if the probability is greater than 0.1 we say that particular patient has heart disease. Now as we vary the threshold it is obvious that prediction will also vary. For example, if the threshold is 0, we will predict every sample in the data is positive (since, probability is greater than equals to 0). Similarly, if we have a high threshold of say 0.9, even though the patient has heart disease it is possible that prediction will be ‘no heart disease’ which is very risky! So we have to choose a threshold in such a way that not only we must have good predictions but also balance the tradeoff. Now we give a summary. Table.1 shows the possible results when applying a classiﬁer to the data, this table is also known as confusion matrix. Now, let us define terms given in the table require to build ROC curve. True positive (TP): Given a patient’s information (age, sex… values), if your model predicts heart disease, and the patient actually has heart disease then, it is considered a true positive. True negative (TN): Given a patient’s information, if your model predicts no heart disease, and the patient actually has no heart disease then, it is considered a true negative. In simple words, if your model correctly predicts positive class, it is true positive, and if your model correctly predicts negative class, it is a true negative. False positive (FP): Given a patient’s information, if your model predicts heart disease, and the patient actually has no heart disease then, it is considered a false positive. False negative(FN): Given a patient’s information, if your model predicts no heart disease, and the patient actually has heart disease then, it is considered a false negative. In simple words, if your model incorrectly (or falsely) predicts positive class, it is a false positive. If your model incorrectly (or falsely) predicts negative class, it is a false negative. Now, TPR/Sensitivity is defined as: TP / P Where P = TP+FN TPR is ‘The fraction of patients with heart disease which are correctly identified’. FPR/False Positive Rate is defined as: FP / N FPR is ‘The fraction of patients without heart disease which are incorrectly identified as with heart disease’. Specificity is defined as: TN / N Where N = TN+FP Specificity is ‘The fraction of patients without heart disease which are correctly identified’. ROC provides a simple way to summarize the information related to different thresholds and resulting TPR and FPR values.Table.2 illustrates TPR and FPR values for different thresholds. You might wonder why some threshold values are negative? I will explain this later. Now, Rather than building different tables (like Table.1.) for different values of threshold, you can just look at ROC curve to decide what threshold to select. Most of the time, the top-left value on ROC curve should give you a quite good threshold, as illustrated in Fig.1, and the corresponding threshold value is highlighted in Table.2. Depend on how many false positive you are willing to accept, you decide optimal threshold. For example, if you don’t want to have too many false positives you should have a high threshold value. This will however, also give you a lot more false negatives. Observe the trade-off and select the best threshold, by decreasing the threshold value of TPR, FPR increases and specificity decreases. Similarly, when we increase the threshold TPR, FPR decreases but specificity increases. AUC makes it easy to compare one ROC curve to another, larger the area under the curve the better the model. In Fig.2.The AUC for SVM with gamma is equals to 0.001is 0.88, the AUC for SVM with gamma is equals to 0.0001 is 0.76, and the AUC for SVM with gamma is equals to 0.00001 is 0.75. We say SVM with gamma is equals to 0.001 is better model than others, since, 0.88 is close to maximum value of AUC that is one, AUC corresponds to SVM with gamma is equals to 0.001 is illustrated in Fig.1., we expect a classifier that performs no better than chance to have an AUC of 0.5, the no information classifier in Fig.2 (red line) predicts every patient as ’with heart disease’ independent of actual target (class). You can also compare other classification models like: Logistic Regression, KNN classifier with SVM. AUC is a widely used metric for binary classification tasks in the industry, and a metric everyone should know about. Note: In general we use probabilities for comparison with the threshold. In case of SVM, decision function values are computed and are compared with the threshold, and can take positive or negative values, which can be seen in Table.2. But the prediction is done in the same way: if function value greater than threshold we assign the sample to the positive class (patient predicted as, ‘with disease’), otherwise we assign the sample to negative class (patient predicted as, ‘without disease’). References: Approaching (Almost) Any Machine Learning Problem, book by Abhishek Thakur. An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie, Daniela Witten.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://clublinguistics.github.io/blogs/images/Fig_2_to_upload.png" /><media:content medium="image" url="https://clublinguistics.github.io/blogs/images/Fig_2_to_upload.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Jack Bogle’s Timeless Investment Principles</title><link href="https://clublinguistics.github.io/blogs/investing/2020/10/10/jack-investing.html" rel="alternate" type="text/html" title="Jack Bogle’s Timeless Investment Principles" /><published>2020-10-10T00:00:00-05:00</published><updated>2020-10-10T00:00:00-05:00</updated><id>https://clublinguistics.github.io/blogs/investing/2020/10/10/jack-investing</id><content type="html" xml:base="https://clublinguistics.github.io/blogs/investing/2020/10/10/jack-investing.html">&lt;p&gt;Jack Bogle was the founder and chief executive of The Vanguard Group, one of the world’s largest investment companies and he is also regarded as the father of index funds. Below are the principles given by Jack in his book Common Sense on Mutual Funds.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Invest you must&lt;/strong&gt;. The biggest risk is the long-term risk of not putting your money to work at a generous return, not the short-term but real risk of price volatility.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Time is your friend&lt;/strong&gt;. Give yourself all the time you can. Begin to invest in your 20s, even if it’s only a small amount, and never stop. Even modest investments in tough times will help you sustain the pace and will become a habit. Compound interest is a miracle.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Impulse is your enemy&lt;/strong&gt;. Eliminate emotion from your investment program. Have rational expectations about future returns, and avoid changing those expectations as the seasons change. Cold, dark winters will give way to bright, bountiful springs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Basic arithmetic works&lt;/strong&gt;. Keep your investment expenses under control. Your net return is simply the gross return of your investment portfolio, less the costs you incur (sales commissions, advisory fees, transaction costs). Low costs make your task easier.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stick to simplicity&lt;/strong&gt;. Don’t complicate the process. Basic investing is simple — a sensible asset allocation to stocks, bonds, and cash reserves; a selection of middle-of-the-road funds that emphasize high-grade securities; a careful balancing of risk, return, and (lest we forget) cost.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stay the course&lt;/strong&gt;. No matter what happens, stick to your program. Jack regarded it as the single most important piece of investment wisdom.&lt;/p&gt;</content><author><name>Saurav Jadhav</name></author><category term="Investing" /><summary type="html">Jack Bogle was the founder and chief executive of The Vanguard Group, one of the world’s largest investment companies and he is also regarded as the father of index funds. Below are the principles given by Jack in his book Common Sense on Mutual Funds.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://clublinguistics.github.io/blogs/images/markus-spiske-investing.jpg" /><media:content medium="image" url="https://clublinguistics.github.io/blogs/images/markus-spiske-investing.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What Is K-Means Clustering?</title><link href="https://clublinguistics.github.io/blogs/machine%20learning/2020/10/04/k_means.html" rel="alternate" type="text/html" title="What Is K-Means Clustering?" /><published>2020-10-04T00:00:00-05:00</published><updated>2020-10-04T00:00:00-05:00</updated><id>https://clublinguistics.github.io/blogs/machine%20learning/2020/10/04/k_means</id><content type="html" xml:base="https://clublinguistics.github.io/blogs/machine%20learning/2020/10/04/k_means.html">&lt;p&gt;&lt;strong&gt;It refers to a very broad set of techniques for finding subgroups, or clusters in a data set&lt;/strong&gt;. When we cluster the observations of a dataset we seek to partition them into distinct groups so that the &lt;strong&gt;observations within each group are quite similar to each other&lt;/strong&gt;, while observations in different groups are quite different. An application of clustering arises in marketing. We may have access to a large number of measurements (example:median household income, occupation, distance from the nearest urban area, and so forth) for a large number of people. Our goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product. The task of performing market segmentation amounts to clustering the people in the data set.&lt;/p&gt;

&lt;p&gt;In general, we can cluster observations on the basis of the features in order to identify subgroups among the observations, or we can cluster features on the basis of the observations in order to discover subgroups among the features.&lt;/p&gt;

&lt;p&gt;Since clustering is popular in many ﬁelds, there exist a great number of clustering methods. But, we will be specifically focusing on K-means clustering, where we seek to partition the observation into &lt;strong&gt;pre-specified number of clusters&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;K-means clustering is a simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters. By non-overlapping we mean that no observation belongs to more than one cluster. To perform K-means clustering, we must ﬁrst specify the desired number of clusters K; then the K-means algorithm will assign each observation to exactly one of the K clusters. Figure.1. shows the results obtained from performing K-means clustering on a simulated example consisting of 150 observations in two dimensions, using three diﬀerent values of K.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/k_means_1.JPG&quot; alt=&quot;&quot; title=&quot;Source: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The idea behind K-means clustering is that a good clustering is one for which the &lt;strong&gt;within-cluster variation is as small as possible&lt;/strong&gt;. That is, the difference between the observations in the same cluster is as small as possible. Here the question arises: &lt;em&gt;How to define within-cluster variation?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There are many possible ways to deﬁne this concept, but by far the most common choice involves &lt;strong&gt;squared Euclidean distance, where we compute the sum of square of differences amongst the observations with in the cluster&lt;/strong&gt;, which gives us magnitude of how close or how far the observations are from each other, now since our goal is to minimize this within cluster variation, this is in fact a very diﬃcult problem to solve precisely, since there are almost K to the power n ways to partition n observations into K clusters. This is a huge number unless K and n are tiny! Fortunately, a very simple algorithm can be shown to provide a local optimum.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Algorithm: K-Means Clustering :&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Randomly assign a number, from 1 to K, to each of the observations. These serve as initial cluster assignments for the observations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Iterate (repeat) until the cluster assignments stop changing:&lt;/p&gt;

    &lt;p&gt;a. For each of the K clusters, compute the cluster centroid. The kth cluster centroid is the vector of the p feature means (average) for the observations in the kth cluster.&lt;/p&gt;

    &lt;p&gt;b. Assign each observation to the cluster whose centroid is closest (where closest is deﬁned using Euclidean distance).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;p&gt;In step 2(a) we compute the mean of the observations within the cluster (cluster centroid). Here an observation is nothing but a vector of p features. Then in step 2(b) we compute distance between the observation to every cluster centroid and then assign the observation to the cluster for which the distance is minimum. This means that as the algorithm is run, the clustering obtained will continually improve until the result no longer changes. When the result no longer changes, a local optimum has been reached. Figure 2 shows the progression of the algorithm on the toy example from Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/k_means_2.JPG&quot; alt=&quot;&quot; title=&quot;Source: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;K-means clustering derives its name from the fact that in Step 2(a), the cluster centroids are computed as the mean of the observations assigned to each cluster. Because the K-means algorithm ﬁnds a local optimum (minimizing the within cluster variance) rather than a global, the results obtained will depend on the initial (random) cluster assignment of each observation in Step 1 of Algorithm. For this reason, it is important to run the algorithm multiple times from diﬀerent random initial conﬁgurations. Then one selects the best solution, that is, &lt;strong&gt;Sum of within cluster variances of all the clusters (objective) is smallest&lt;/strong&gt;. Figure 3 shows the local optima obtained by running K-means clustering six times using six diﬀerent initial cluster assignments, using the toy data from Figure 1. In this case, the best clustering is the one with an objective value of 235.8(least).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/k_means_3.JPG&quot; alt=&quot;&quot; title=&quot;Source: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we have seen, to perform K-means clustering, we must decide how many clusters we expect in the data. The problem of selecting K is far from simple, and we may not know in advance how many clusters to select. Therefore, it is one of the drawbacks of K-means clustering. The mathematics of how clustering is performed is too technical to be discussed here.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt;: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.&lt;/p&gt;</content><author><name>Saurav Jadhav</name></author><category term="machine learning" /><summary type="html">It refers to a very broad set of techniques for finding subgroups, or clusters in a data set. When we cluster the observations of a dataset we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different. An application of clustering arises in marketing. We may have access to a large number of measurements (example:median household income, occupation, distance from the nearest urban area, and so forth) for a large number of people. Our goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product. The task of performing market segmentation amounts to clustering the people in the data set. In general, we can cluster observations on the basis of the features in order to identify subgroups among the observations, or we can cluster features on the basis of the observations in order to discover subgroups among the features. Since clustering is popular in many ﬁelds, there exist a great number of clustering methods. But, we will be specifically focusing on K-means clustering, where we seek to partition the observation into pre-specified number of clusters. K-means clustering is a simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters. By non-overlapping we mean that no observation belongs to more than one cluster. To perform K-means clustering, we must ﬁrst specify the desired number of clusters K; then the K-means algorithm will assign each observation to exactly one of the K clusters. Figure.1. shows the results obtained from performing K-means clustering on a simulated example consisting of 150 observations in two dimensions, using three diﬀerent values of K. The idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. That is, the difference between the observations in the same cluster is as small as possible. Here the question arises: How to define within-cluster variation? There are many possible ways to deﬁne this concept, but by far the most common choice involves squared Euclidean distance, where we compute the sum of square of differences amongst the observations with in the cluster, which gives us magnitude of how close or how far the observations are from each other, now since our goal is to minimize this within cluster variation, this is in fact a very diﬃcult problem to solve precisely, since there are almost K to the power n ways to partition n observations into K clusters. This is a huge number unless K and n are tiny! Fortunately, a very simple algorithm can be shown to provide a local optimum. Algorithm: K-Means Clustering : Randomly assign a number, from 1 to K, to each of the observations. These serve as initial cluster assignments for the observations. Iterate (repeat) until the cluster assignments stop changing: a. For each of the K clusters, compute the cluster centroid. The kth cluster centroid is the vector of the p feature means (average) for the observations in the kth cluster. b. Assign each observation to the cluster whose centroid is closest (where closest is deﬁned using Euclidean distance). In step 2(a) we compute the mean of the observations within the cluster (cluster centroid). Here an observation is nothing but a vector of p features. Then in step 2(b) we compute distance between the observation to every cluster centroid and then assign the observation to the cluster for which the distance is minimum. This means that as the algorithm is run, the clustering obtained will continually improve until the result no longer changes. When the result no longer changes, a local optimum has been reached. Figure 2 shows the progression of the algorithm on the toy example from Figure 1. K-means clustering derives its name from the fact that in Step 2(a), the cluster centroids are computed as the mean of the observations assigned to each cluster. Because the K-means algorithm ﬁnds a local optimum (minimizing the within cluster variance) rather than a global, the results obtained will depend on the initial (random) cluster assignment of each observation in Step 1 of Algorithm. For this reason, it is important to run the algorithm multiple times from diﬀerent random initial conﬁgurations. Then one selects the best solution, that is, Sum of within cluster variances of all the clusters (objective) is smallest. Figure 3 shows the local optima obtained by running K-means clustering six times using six diﬀerent initial cluster assignments, using the toy data from Figure 1. In this case, the best clustering is the one with an objective value of 235.8(least). As we have seen, to perform K-means clustering, we must decide how many clusters we expect in the data. The problem of selecting K is far from simple, and we may not know in advance how many clusters to select. Therefore, it is one of the drawbacks of K-means clustering. The mathematics of how clustering is performed is too technical to be discussed here. Reference: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://clublinguistics.github.io/blogs/images/k_means_1.JPG" /><media:content medium="image" url="https://clublinguistics.github.io/blogs/images/k_means_1.JPG" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Waste Is Future OR Not ? An effective way to treat plastic waste</title><link href="https://clublinguistics.github.io/blogs/environment/social%20issues/chemistry/2020/09/29/plastic-waste.html" rel="alternate" type="text/html" title="Waste Is Future OR Not ? An effective way to treat plastic waste" /><published>2020-09-29T00:00:00-05:00</published><updated>2020-09-29T00:00:00-05:00</updated><id>https://clublinguistics.github.io/blogs/environment/social%20issues/chemistry/2020/09/29/plastic-waste</id><content type="html" xml:base="https://clublinguistics.github.io/blogs/environment/social%20issues/chemistry/2020/09/29/plastic-waste.html">&lt;p&gt;The rapid rate of increase in plastic has led to the creation of increasing amounts of waste and this in turn poses greater difficulties for disposal.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/global-primary-plastic.png&quot; alt=&quot;&quot; title=&quot;Global primary plastics waste generation 1950–2015&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is clearly observed from the above figure that plastic waste generation increases year by year ,&lt;strong&gt;300 million tonnes&lt;/strong&gt; of plastic waste was generated alone in 2015. Huge amount of waste plastic arise as a byproduct or faulty product in industry and agriculture. The plastic are mainly of two types : thermoplastics and thermosetting plastics. Thermo plastics are composed of polyolefin such as high density polyethylene (HDPE), low density polyethylene (LDPE), polypropylene (PP), polystyrene (PS), polyethylene terephthalate (PET) and polyvinyl chloride (PVC) and can be recycled. Thermosetting plastics mainly include epoxy resins and polyurethanes and cannot be recycled. Plastic wastes are the main causes of environmental pollution. There are three ways to manage plastic wastes: land filling, incineration and recycling. The problem of waste cannot be solved by landfilling and incineration, in landfilling we dumb the waste on the ground which not only consumes space but also pollutes ground water, in incineration it generates harmful greenhouse gases, e.g. NOx, SOx, COx.&lt;/p&gt;

&lt;p&gt;Recycling of plastic is difficult and costly because of the restrictions on contamination of water and labor intensive segregation of different plastics before recycle which is labor intensive. Segregation of different plastic materials are essential since, they are made of different resin compound for difference in transparency and color. Dyed or pigmented plastics have a lower market value too. Clearly transparent plastics can be easily dyed to transform into new products, have greater flexibility and are mostly desirable by the manufacturers. Recycling plastic is energy intensive too. As there is an alarming depletion of energy sources, means of energy recovery from plastic waste is a good option. &lt;strong&gt;Pyrolysis&lt;/strong&gt; is a suitable method for energy recovery from plastic waste and is one of the finest techniques for the conversion of mass to energy with liquid and gaseous products with high energy values. Below Figure represents the processes involved in the pyrolysis of plastic.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/pyrolysis-process.jpeg&quot; alt=&quot;&quot; title=&quot;Flow chart of plastic pyrolysis process&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To understand why plastic waste can be converted into fuel we have to understand the similarities between crude oil and plastic.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Crude oil is a complex mixture of hydrocarbons, which are separated and purified by distillation and other processes at an oil refinery. These oil products are not single components, but are a blend of components used to meet the relevant fuel specifications in the most economic manner, given the composition of the crude oil and the configuration of the oil refinery. Plastic is a generic term for a wide range of polymers produced using highly refined fractions of crude oil, or chemicals derived from crude oil, known as monomers. Polymers are formed by the reaction of these monomers, which results in chain lengths of tens or hundreds of thousands of carbon atoms. It is worth noting that only a small proportion (&amp;lt;5%) of the crude oil processed in the world is used to produce the monomers (example ethane, propene) used in the manufacture of polymers (example polyethene, polypropylene). The similarity between oil products and plastics is illustrated in below figure. The figure demonstrates where the atomic composition in most plastics is similar to those in gasoline and diesel derived from crude oil.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/atomic-composition-fuels.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Various methods can be used for producing oil from plastic waste like: Gasification and hydrogenation. But, pyrolysis is commonly used since, it is economical and effective.&lt;/p&gt;

&lt;p&gt;Pyrolysis is the degradation of long-chain polymer molecules into smaller molecules by intense heating and in the absence of oxygen. It is the thermal decomposition of polymers or substances in the presence of an inert gas (e.g. Nitrogen).&lt;/p&gt;

&lt;p&gt;Catalytic pyrolysis involves the use of a catalyst like activated carbon or zeolite. It produces liquid oil of higher quality at lower residence time and temperature compared to thermal pyrolysis(without catalyst). Catalyst favors the yield of lighter hydrocarbons, gasoline products and gases. It lowers the activation energy of plastic during pyrolysis, thereby lowering the energy requirement. The distribution of the products obtained depends on the type of polymer, their sources and structures.&lt;/p&gt;

&lt;p&gt;Below figure illustrates the output of products from 1 kg of plastic :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/product-quantity-from-plastic-waste.jpeg&quot; alt=&quot;&quot; title=&quot;Source : ANZ Plastic Waste Management Company India&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Depending on the composition of liquid hydrocarbons obtained, after distillation they can be separated into Gasoline, diesel and other fuels. The coke obtained can also be further used to produce activated carbon which has wide variety of applications.&lt;/p&gt;

&lt;p&gt;Americans throw away &lt;strong&gt;35 billion plastic bottles every year&lt;/strong&gt;. Only about 25% of the plastic produced in the U.S. is recycled. If we recycled the other 75% we could save 1 billion gallons of oil and 44 million cubic yards of landfill space annually. Therefore, there is a need for effective utilization of plastic waste to tackle environmental degradation.&lt;/p&gt;

&lt;p&gt;Several research is going on for conversion of different waste such as, textile, wood and rubber into fuels and other valuable products and a lot of area is still unexplored.&lt;/p&gt;</content><author><name>Anup Gawande</name></author><category term="Environment" /><category term="Social Issues" /><category term="Chemistry" /><summary type="html">The rapid rate of increase in plastic has led to the creation of increasing amounts of waste and this in turn poses greater difficulties for disposal.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://clublinguistics.github.io/blogs/images/erik-mclean-plastic-waste_orig.jpg" /><media:content medium="image" url="https://clublinguistics.github.io/blogs/images/erik-mclean-plastic-waste_orig.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What Is SVM?</title><link href="https://clublinguistics.github.io/blogs/machine%20learning/2020/09/09/svm.html" rel="alternate" type="text/html" title="What Is SVM?" /><published>2020-09-09T00:00:00-05:00</published><updated>2020-09-09T00:00:00-05:00</updated><id>https://clublinguistics.github.io/blogs/machine%20learning/2020/09/09/svm</id><content type="html" xml:base="https://clublinguistics.github.io/blogs/machine%20learning/2020/09/09/svm.html">&lt;p&gt;Support Vector Machine (SVM) is an approach for classification which uses the concept of separating hyperplane. It was developed in the 1990s. It is a generalization of an intuitive and simple classifier called maximal margin classifier.&lt;/p&gt;

&lt;p&gt;In order to study Support Vector Machine (SVM), we first need to understand what is maximal margin classifier and support vector classifier.&lt;/p&gt;

&lt;p&gt;In maximal margin classifier, we use a hyperplane to separate the classes. But, &lt;strong&gt;What is a hyperplane?&lt;/strong&gt; Consider we have a p-dimensional space, a hyperplane is a flat affine(does not necessarily pass from the origin)subspace of dimension p-1. For example, in a two-dimensional space, a hyperplane is a one-dimensional flat subspace, which is nothing but a line. Similarly, in a three-dimensional space, a hyperplane is two-dimensional flat subspace which is nothing but a plane. Figure 1 illustrates a hyperplane in two-dimensional space.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/Figure_1_svm.png&quot; alt=&quot;&quot; title=&quot;Source : An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From Figure 1, we can also see this hyperplane as a line dividing the space in two halves. Therefore, it can act as a decision boundary for classification. For example, in right hand panel of Figure 2 the points above the line belong to blue class and points below the line belongs to purple.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/Figure_2_svm.png&quot; alt=&quot;&quot; title=&quot;Source : An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In general, if our data can be perfectly separated using a hyperplane, then there will in fact exist an inﬁnite number of such hyperplanes. This is because a given separating hyperplane can usually be shifted a tiny bit up or down, or rotated, without coming into contact with any of the observations. Three possible separating hyperplanes are shown in the left hand panel of Figure 2. In order to construct a classiﬁer based upon a separating hyperplane, we must have a reasonable way to decide which of the inﬁnite possible separating hyperplanes to use. A natural choice is the &lt;strong&gt;maximal margin hyperplane&lt;/strong&gt; (also known as the optimal separating hyperplane), which is the separating hyperplane that is farthest from the training observations. That is, we can compute the (perpendicular) distance from each training observation to a given separating hyperplane; the smallest such distance is the minimal distance from the observations to the hyperplane, and is known as the margin. &lt;strong&gt;The maximal margin hyperplane is the separating hyperplane for which the margin is largest&lt;/strong&gt; — that is, it is the hyperplane that has the farthest minimum distance to the training observations. We can then classify a test observation based on which side of the maximal margin hyperplane it lies. This is known as the maximal margin classiﬁer. Figure 3 shows the maximal margin hyperplane on the data which is used in Figure 2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/Figure_3_svm.png&quot; alt=&quot;&quot; title=&quot;Source : An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Comparing right hand panel of Figure 2 to Figure 3, we see that the maximal margin hyperplane shown in Figure 3 does indeed result in a greater minimal distance between the observations and the separating hyperplane — that is, a larger margin. In a sense, &lt;strong&gt;the maximal margin hyperplane represents the mid-line of the widest “slab” that we can insert between the two classes&lt;/strong&gt;. Examining Figure 3, we see that three training observations are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin. These three observations are known as support vectors, since they are vectors in p-dimensional space (in Figure 3, p = 2) and they “support” the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyperplane would move as well. Interestingly, &lt;strong&gt;the maximal margin hyperplane depends directly on the support vectors, but not on the other observations&lt;/strong&gt;: a movement to any of the other observations would not aﬀect the separating hyperplane, provided that the observation’s movement does not cause it to cross the boundary set by the margin.&lt;/p&gt;

&lt;p&gt;The maximal margin hyperplane requires the existence of separating hyperplane but, in many cases no separating hyperplane exists. That is, we cannot exactly separate the two classes using a hyperplane. An example for such is given in Figure 4.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/Figure_4_svm.png&quot; alt=&quot;&quot; title=&quot;Source : An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, we cannot exactly separate the two classes. However, we can extend the concept of a separating hyperplane in order to develop a hyperplane that almost separates the classes, using a so-called &lt;strong&gt;soft margin. The generalization of the maximal margin classiﬁer to the non-separable case is known as the support vector classiﬁer&lt;/strong&gt;. In Figure 4, we see that observations that belong to two classes are not necessarily separable by a hyperplane. In fact, even if a separating hyperplane does exist, then there are instances in which a classiﬁer based on a separating hyperplane might not be desirable. A classiﬁer based on a separating hyperplane will necessarily perfectly classify all of the training observations; this can lead to sensitivity to individual observations. An example is shown in Figure 5.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/Figure_5_svm.png&quot; alt=&quot;&quot; title=&quot;Source : An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The addition of a single observation in the right-hand panel of Figure 5 leads to a dramatic change in the maximal margin hyperplane. The resulting maximal margin hyperplane is not satisfactory — for one thing, it has only a tiny margin. This is problematic because as discussed previously, the distance of an observation from the hyperplane can be seen as a measure of our conﬁdence that the observation was correctly classiﬁed. Therefore, it could be worthwhile to misclassify a few training observations in order to do a better job in classifying the remaining observations. The support vector classiﬁer, sometimes called a soft margin classiﬁer, does exactly this. &lt;strong&gt;Rather than seeking the largest possible margin so that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observations to be on the incorrect side of the margin, or even the incorrect side of the hyperplane&lt;/strong&gt;. (The margin is soft because it can be violated by some of the training observations.) An example is shown in the left hand panel of Figure 6.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/Figure_6_svm.png&quot; alt=&quot;&quot; title=&quot;Source : An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most of the observations are on the correct side of the margin, but a small set of observations are on the wrong side of the margin(observation 1 and 8). An observation can not only be on the wrong side of margin but also on the wrong side of the hyperplane. In case no hyperplane exist then such a scenario is inevitable. Observations corresponds to the wrong side of the hyperplane are observations that are misclassified by support vector classifier. &lt;strong&gt;Observations which lie on correct side of the margin does not affect the support vector classifier, but the observations that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors&lt;/strong&gt;. These observations do aﬀect the support vector classiﬁer.&lt;/p&gt;

&lt;p&gt;The support vector classiﬁer is a natural approach for classiﬁcation in the two-class setting, if the boundary between the two classes is linear. However, in practice we are sometimes faced with non-linear class boundaries. For instance consider the data shown in left panel of Figure 7.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/Figure_7_svm.png&quot; alt=&quot;&quot; title=&quot;Source : An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is clear that a support vector classiﬁer or any linear classiﬁer will perform poorly here. Indeed, the support vector classiﬁer shown in the right-hand panel of Figure 7 is useless here.&lt;/p&gt;

&lt;p&gt;We can address this problem by enlarging the feature space using quadratic, cubic or higher order polynomial functions of the features. The &lt;strong&gt;support vector machine (SVM)&lt;/strong&gt; is an extension of the support vector classiﬁer that results from enlarging the feature space in a speciﬁc way, using kernels. For example, In Figure 7 we have two features X1 and X2, rather than using these features as it is for classification, we can also include higher degree terms of these features. Such as, X1² and X2² , which will give us a quadratic polynomial whose solution is nonlinear. We use &lt;strong&gt;kernels&lt;/strong&gt; for doing exactly this in an efficient manner, where we specify what kind of decision boundary to use. For example: linear, polynomial (with some degree) and radial. Figure 8 illustrates the use of polynomial and radial kernel on the data of Figure 7.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blogs/images/Figure_8_svm.png&quot; alt=&quot;&quot; title=&quot;Source : An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the left hand panel we used a polynomial kernel with degree 3 and on the right we have used a radial kernel. Both the kernels resulted in more appropriate decision rule. The mathematics of how decision boundaries and kernels are obtained, are too technical to discuss here.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt;: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.&lt;/p&gt;</content><author><name>Saurav Jadhav</name></author><category term="machine learning" /><summary type="html">Support Vector Machine (SVM) is an approach for classification which uses the concept of separating hyperplane. It was developed in the 1990s. It is a generalization of an intuitive and simple classifier called maximal margin classifier. In order to study Support Vector Machine (SVM), we first need to understand what is maximal margin classifier and support vector classifier. In maximal margin classifier, we use a hyperplane to separate the classes. But, What is a hyperplane? Consider we have a p-dimensional space, a hyperplane is a flat affine(does not necessarily pass from the origin)subspace of dimension p-1. For example, in a two-dimensional space, a hyperplane is a one-dimensional flat subspace, which is nothing but a line. Similarly, in a three-dimensional space, a hyperplane is two-dimensional flat subspace which is nothing but a plane. Figure 1 illustrates a hyperplane in two-dimensional space. From Figure 1, we can also see this hyperplane as a line dividing the space in two halves. Therefore, it can act as a decision boundary for classification. For example, in right hand panel of Figure 2 the points above the line belong to blue class and points below the line belongs to purple. In general, if our data can be perfectly separated using a hyperplane, then there will in fact exist an inﬁnite number of such hyperplanes. This is because a given separating hyperplane can usually be shifted a tiny bit up or down, or rotated, without coming into contact with any of the observations. Three possible separating hyperplanes are shown in the left hand panel of Figure 2. In order to construct a classiﬁer based upon a separating hyperplane, we must have a reasonable way to decide which of the inﬁnite possible separating hyperplanes to use. A natural choice is the maximal margin hyperplane (also known as the optimal separating hyperplane), which is the separating hyperplane that is farthest from the training observations. That is, we can compute the (perpendicular) distance from each training observation to a given separating hyperplane; the smallest such distance is the minimal distance from the observations to the hyperplane, and is known as the margin. The maximal margin hyperplane is the separating hyperplane for which the margin is largest — that is, it is the hyperplane that has the farthest minimum distance to the training observations. We can then classify a test observation based on which side of the maximal margin hyperplane it lies. This is known as the maximal margin classiﬁer. Figure 3 shows the maximal margin hyperplane on the data which is used in Figure 2. Comparing right hand panel of Figure 2 to Figure 3, we see that the maximal margin hyperplane shown in Figure 3 does indeed result in a greater minimal distance between the observations and the separating hyperplane — that is, a larger margin. In a sense, the maximal margin hyperplane represents the mid-line of the widest “slab” that we can insert between the two classes. Examining Figure 3, we see that three training observations are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin. These three observations are known as support vectors, since they are vectors in p-dimensional space (in Figure 3, p = 2) and they “support” the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyperplane would move as well. Interestingly, the maximal margin hyperplane depends directly on the support vectors, but not on the other observations: a movement to any of the other observations would not aﬀect the separating hyperplane, provided that the observation’s movement does not cause it to cross the boundary set by the margin. The maximal margin hyperplane requires the existence of separating hyperplane but, in many cases no separating hyperplane exists. That is, we cannot exactly separate the two classes using a hyperplane. An example for such is given in Figure 4. In this case, we cannot exactly separate the two classes. However, we can extend the concept of a separating hyperplane in order to develop a hyperplane that almost separates the classes, using a so-called soft margin. The generalization of the maximal margin classiﬁer to the non-separable case is known as the support vector classiﬁer. In Figure 4, we see that observations that belong to two classes are not necessarily separable by a hyperplane. In fact, even if a separating hyperplane does exist, then there are instances in which a classiﬁer based on a separating hyperplane might not be desirable. A classiﬁer based on a separating hyperplane will necessarily perfectly classify all of the training observations; this can lead to sensitivity to individual observations. An example is shown in Figure 5. The addition of a single observation in the right-hand panel of Figure 5 leads to a dramatic change in the maximal margin hyperplane. The resulting maximal margin hyperplane is not satisfactory — for one thing, it has only a tiny margin. This is problematic because as discussed previously, the distance of an observation from the hyperplane can be seen as a measure of our conﬁdence that the observation was correctly classiﬁed. Therefore, it could be worthwhile to misclassify a few training observations in order to do a better job in classifying the remaining observations. The support vector classiﬁer, sometimes called a soft margin classiﬁer, does exactly this. Rather than seeking the largest possible margin so that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observations to be on the incorrect side of the margin, or even the incorrect side of the hyperplane. (The margin is soft because it can be violated by some of the training observations.) An example is shown in the left hand panel of Figure 6. Most of the observations are on the correct side of the margin, but a small set of observations are on the wrong side of the margin(observation 1 and 8). An observation can not only be on the wrong side of margin but also on the wrong side of the hyperplane. In case no hyperplane exist then such a scenario is inevitable. Observations corresponds to the wrong side of the hyperplane are observations that are misclassified by support vector classifier. Observations which lie on correct side of the margin does not affect the support vector classifier, but the observations that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors. These observations do aﬀect the support vector classiﬁer. The support vector classiﬁer is a natural approach for classiﬁcation in the two-class setting, if the boundary between the two classes is linear. However, in practice we are sometimes faced with non-linear class boundaries. For instance consider the data shown in left panel of Figure 7. It is clear that a support vector classiﬁer or any linear classiﬁer will perform poorly here. Indeed, the support vector classiﬁer shown in the right-hand panel of Figure 7 is useless here. We can address this problem by enlarging the feature space using quadratic, cubic or higher order polynomial functions of the features. The support vector machine (SVM) is an extension of the support vector classiﬁer that results from enlarging the feature space in a speciﬁc way, using kernels. For example, In Figure 7 we have two features X1 and X2, rather than using these features as it is for classification, we can also include higher degree terms of these features. Such as, X1² and X2² , which will give us a quadratic polynomial whose solution is nonlinear. We use kernels for doing exactly this in an efficient manner, where we specify what kind of decision boundary to use. For example: linear, polynomial (with some degree) and radial. Figure 8 illustrates the use of polynomial and radial kernel on the data of Figure 7. On the left hand panel we used a polynomial kernel with degree 3 and on the right we have used a radial kernel. Both the kernels resulted in more appropriate decision rule. The mathematics of how decision boundaries and kernels are obtained, are too technical to discuss here. Reference: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://clublinguistics.github.io/blogs/images/Figure_3_svm.png" /><media:content medium="image" url="https://clublinguistics.github.io/blogs/images/Figure_3_svm.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>