{
  
    
        "post0": {
            "title": "What Is ROC Curve?",
            "content": "What Is ROC Curve? . ROC curve is an evaluation metric which measures the performance of a machine learning model by visualizing, especially when data is skewed. Let’s see what exactly that means, . Consider heart data which consist of 13 features such as age, sex, chol (cholesterol measurement). Our goal is to predict whether an individual has heart disease based on the above features, this is a binary classification problem. That is, with only two classes. Let us suppose we have 100 samples (a sample corresponds to patient information) amongst which 90 samples are positive (have a heart disease) so if you say that all patients (100 of them)have heart disease, you have correctly classified 90/100 samples. You didn’t even build the model and got an accuracy of 90%. But, if we look carefully we will see that the dataset is skewed. That is, number of positive samples is far more than the negative samples (patients without heart disease). So, it is not advisable to decide best model just on the basis of accuracy because, it does not represent the data completely. So, you might get high accuracy, but your model will probably not perform that well when it comes to real-world samples. Therefore, we need a more reliable evaluation metric and hence, ROC comes into the picture. . Here we will use SVM (support vector machine) as model with different values of gamma (parameter) for comparison and with the help of ROC curve figure out which value of gamma gives us the optimal result (best model). Let’s see this in detail. . Rather than predicting samples are positive or not, we predict the probability of having heart disease for each sample, and if this probability is greater than the threshold, we say the given patient has heart disease. So let’s say we select a threshold value of 0.1 therefore, if the probability is greater than 0.1 we say that particular patient has heart disease. Now as we vary the threshold it is obvious that prediction will also vary. For example, if the threshold is 0, we will predict every sample in the data is positive (since, probability is greater than equals to 0). Similarly, if we have a high threshold of say 0.9, even though the patient has heart disease it is possible that prediction will be ‘no heart disease’ which is very risky! So we have to choose a threshold in such a way that not only we must have good predictions but also balance the tradeoff. Now we give a summary. Table.1 shows the possible results when applying a classiﬁer to the data, this table is also known as confusion matrix. Now, let us define terms given in the table require to build ROC curve. . . True positive (TP): Given a patient’s information (age, sex… values), if your model predicts heart disease, and the patient actually has heart disease then, it is considered a true positive. . True negative (TN): Given a patient’s information, if your model predicts no heart disease, and the patient actually has no heart disease then, it is considered a true negative. . In simple words, if your model correctly predicts positive class, it is true positive, and if your model correctly predicts negative class, it is a true negative. . False positive (FP): Given a patient’s information, if your model predicts heart disease, and the patient actually has no heart disease then, it is considered a false positive. . False negative(FN): Given a patient’s information, if your model predicts no heart disease, and the patient actually has heart disease then, it is considered a false negative. . In simple words, if your model incorrectly (or falsely) predicts positive class, it is a false positive. If your model incorrectly (or falsely) predicts negative class, it is a false negative. Now, . TPR/Sensitivity is defined as: . TP / P . Where P = TP+FN . TPR is ‘The fraction of patients with heart disease which are correctly identified’. . FPR/False Positive Rate is defined as: . FP / N . FPR is ‘The fraction of patients without heart disease which are incorrectly identified as with heart disease’. . Specificity is defined as: . TN / N . Where N = TN+FP . Specificity is ‘The fraction of patients without heart disease which are correctly identified’. . ROC provides a simple way to summarize the information related to different thresholds and resulting TPR and FPR values.Table.2 illustrates TPR and FPR values for different thresholds. . . You might wonder why some threshold values are negative? I will explain this later. Now, Rather than building different tables (like Table.1.) for different values of threshold, you can just look at ROC curve to decide what threshold to select. Most of the time, the top-left value on ROC curve should give you a quite good threshold, as illustrated in Fig.1, and the corresponding threshold value is highlighted in Table.2. . . Depend on how many false positive you are willing to accept, you decide optimal threshold. For example, if you don’t want to have too many false positives you should have a high threshold value. This will however, also give you a lot more false negatives. Observe the trade-off and select the best threshold, by decreasing the threshold value of TPR, FPR increases and specificity decreases. Similarly, when we increase the threshold TPR, FPR decreases but specificity increases. . AUC makes it easy to compare one ROC curve to another, larger the area under the curve the better the model. . . In Fig.2.The AUC for SVM with gamma is equals to 0.001is 0.88, the AUC for SVM with gamma is equals to 0.0001 is 0.76, and the AUC for SVM with gamma is equals to 0.00001 is 0.75. We say SVM with gamma is equals to 0.001 is better model than others, since, 0.88 is close to maximum value of AUC that is one, AUC corresponds to SVM with gamma is equals to 0.001 is illustrated in Fig.1., we expect a classifier that performs no better than chance to have an AUC of 0.5, the no information classifier in Fig.2 (red line) predicts every patient as ’with heart disease’ independent of actual target (class). . You can also compare other classification models like: Logistic Regression, KNN classifier with SVM. . AUC is a widely used metric for binary classification tasks in the industry, and a metric everyone should know about. . Note: In general we use probabilities for comparison with the threshold. In case of SVM, decision function values are computed and are compared with the threshold, and can take positive or negative values, which can be seen in Table.2. But the prediction is done in the same way: if function value greater than threshold we assign the sample to the positive class (patient predicted as, ‘with disease’), otherwise we assign the sample to negative class (patient predicted as, ‘without disease’). . References: . Approaching (Almost) Any Machine Learning Problem, book by Abhishek Thakur. | An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie, Daniela Witten. |",
            "url": "https://clublinguistics.github.io/blogs/machine%20learning/2020/12/26/roc.html",
            "relUrl": "/machine%20learning/2020/12/26/roc.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "K_means",
            "content": "What Is K-Means Clustering? . When it is your first day at school, you meet people you barely know and after spending days, you become friends with some of them, based on similarities. Clustering is exactly this. . It refers to a very broad set of techniques for finding subgroups, or clusters in a data set. When we cluster the observations of a dataset we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different. An application of clustering arises in marketing. We may have access to a large number of measurements (example:median household income, occupation, distance from the nearest urban area, and so forth) for a large number of people. Our goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product. The task of performing market segmentation amounts to clustering the people in the data set. . In general, we can cluster observations on the basis of the features in order to identify subgroups among the observations, or we can cluster features on the basis of the observations in order to discover subgroups among the features. . Since clustering is popular in many ﬁelds, there exist a great number of clustering methods. But, we will be specifically focusing on K-means clustering, where we seek to partition the observation into pre-specified number of clusters. . K-means clustering is a simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters. By non-overlapping we mean that no observation belongs to more than one cluster. To perform K-means clustering, we must ﬁrst specify the desired number of clusters K; then the K-means algorithm will assign each observation to exactly one of the K clusters. Figure.1. shows the results obtained from performing K-means clustering on a simulated example consisting of 150 observations in two dimensions, using three diﬀerent values of K. . . The idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. That is, the difference between the observations in the same cluster is as small as possible. Here the question arises: How to define within-cluster variation? . There are many possible ways to deﬁne this concept, but by far the most common choice involves squared Euclidean distance, where we compute the sum of square of differences amongst the observations with in the cluster, which gives us magnitude of how close or how far the observations are from each other, now since our goal is to minimize this within cluster variation, this is in fact a very diﬃcult problem to solve precisely, since there are almost K to the power n ways to partition n observations into K clusters. This is a huge number unless K and n are tiny! Fortunately, a very simple algorithm can be shown to provide a local optimum. . . Algorithm: K-Means Clustering : . Randomly assign a number, from 1 to K, to each of the observations. These serve as initial cluster assignments for the observations. . | Iterate (repeat) until the cluster assignments stop changing: . a. For each of the K clusters, compute the cluster centroid. The kth cluster centroid is the vector of the p feature means (average) for the observations in the kth cluster. . b. Assign each observation to the cluster whose centroid is closest (where closest is deﬁned using Euclidean distance). . | . In step 2(a) we compute the mean of the observations within the cluster (cluster centroid). Here an observation is nothing but a vector of p features. Then in step 2(b) we compute distance between the observation to every cluster centroid and then assign the observation to the cluster for which the distance is minimum. This means that as the algorithm is run, the clustering obtained will continually improve until the result no longer changes. When the result no longer changes, a local optimum has been reached. Figure 2 shows the progression of the algorithm on the toy example from Figure 1. . . K-means clustering derives its name from the fact that in Step 2(a), the cluster centroids are computed as the mean of the observations assigned to each cluster. Because the K-means algorithm ﬁnds a local optimum (minimizing the within cluster variance) rather than a global, the results obtained will depend on the initial (random) cluster assignment of each observation in Step 1 of Algorithm. For this reason, it is important to run the algorithm multiple times from diﬀerent random initial conﬁgurations. Then one selects the best solution, that is, Sum of within cluster variances of all the clusters (objective) is smallest. Figure 3 shows the local optima obtained by running K-means clustering six times using six diﬀerent initial cluster assignments, using the toy data from Figure 1. In this case, the best clustering is the one with an objective value of 235.8(least). . . As we have seen, to perform K-means clustering, we must decide how many clusters we expect in the data. The problem of selecting K is far from simple, and we may not know in advance how many clusters to select. Therefore, it is one of the drawbacks of K-means clustering. The mathematics of how clustering is performed is too technical to be discussed here. . Reference: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten. .",
            "url": "https://clublinguistics.github.io/blogs/2020/10/04/k_means.html",
            "relUrl": "/2020/10/04/k_means.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Svm",
            "content": "What Is SVM? . Support Vector Machine (SVM) is an approach for classification which uses the concept of separating hyperplane. It was developed in the 1990s. It is a generalization of an intuitive and simple classifier called maximal margin classifier. . In order to study Support Vector Machine (SVM), we first need to understand what is maximal margin classifier and support vector classifier. . In maximal margin classifier, we use a hyperplane to separate the classes. But, What is a hyperplane? Consider we have a p-dimensional space, a hyperplane is a flat affine(does not necessarily pass from the origin)subspace of dimension p-1. For example, in a two-dimensional space, a hyperplane is a one-dimensional flat subspace, which is nothing but a line. Similarly, in a three-dimensional space, a hyperplane is two-dimensional flat subspace which is nothing but a plane. Figure 1 illustrates a hyperplane in two-dimensional space. . . From Figure 1, we can also see this hyperplane as a line dividing the space in two halves. Therefore, it can act as a decision boundary for classification. For example, in right hand panel of Figure 2 the points above the line belong to blue class and points below the line belongs to purple. . . In general, if our data can be perfectly separated using a hyperplane, then there will in fact exist an inﬁnite number of such hyperplanes. This is because a given separating hyperplane can usually be shifted a tiny bit up or down, or rotated, without coming into contact with any of the observations. Three possible separating hyperplanes are shown in the left hand panel of Figure 2. In order to construct a classiﬁer based upon a separating hyperplane, we must have a reasonable way to decide which of the inﬁnite possible separating hyperplanes to use. A natural choice is the maximal margin hyperplane (also known as the optimal separating hyperplane), which is the separating hyperplane that is farthest from the training observations. That is, we can compute the (perpendicular) distance from each training observation to a given separating hyperplane; the smallest such distance is the minimal distance from the observations to the hyperplane, and is known as the margin. The maximal margin hyperplane is the separating hyperplane for which the margin is largest — that is, it is the hyperplane that has the farthest minimum distance to the training observations. We can then classify a test observation based on which side of the maximal margin hyperplane it lies. This is known as the maximal margin classiﬁer. Figure 3 shows the maximal margin hyperplane on the data which is used in Figure 2. . . Comparing right hand panel of Figure 2 to Figure 3, we see that the maximal margin hyperplane shown in Figure 3 does indeed result in a greater minimal distance between the observations and the separating hyperplane — that is, a larger margin. In a sense, the maximal margin hyperplane represents the mid-line of the widest “slab” that we can insert between the two classes. Examining Figure 3, we see that three training observations are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin. These three observations are known as support vectors, since they are vectors in p-dimensional space (in Figure 3, p = 2) and they “support” the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyperplane would move as well. Interestingly, the maximal margin hyperplane depends directly on the support vectors, but not on the other observations: a movement to any of the other observations would not aﬀect the separating hyperplane, provided that the observation’s movement does not cause it to cross the boundary set by the margin. . The maximal margin hyperplane requires the existence of separating hyperplane but, in many cases no separating hyperplane exists. That is, we cannot exactly separate the two classes using a hyperplane. An example for such is given in Figure 4. . . In this case, we cannot exactly separate the two classes. However, we can extend the concept of a separating hyperplane in order to develop a hyperplane that almost separates the classes, using a so-called soft margin. The generalization of the maximal margin classiﬁer to the non-separable case is known as the support vector classiﬁer. In Figure 4, we see that observations that belong to two classes are not necessarily separable by a hyperplane. In fact, even if a separating hyperplane does exist, then there are instances in which a classiﬁer based on a separating hyperplane might not be desirable. A classiﬁer based on a separating hyperplane will necessarily perfectly classify all of the training observations; this can lead to sensitivity to individual observations. An example is shown in Figure 5. . . The addition of a single observation in the right-hand panel of Figure 5 leads to a dramatic change in the maximal margin hyperplane. The resulting maximal margin hyperplane is not satisfactory — for one thing, it has only a tiny margin. This is problematic because as discussed previously, the distance of an observation from the hyperplane can be seen as a measure of our conﬁdence that the observation was correctly classiﬁed. Therefore, it could be worthwhile to misclassify a few training observations in order to do a better job in classifying the remaining observations. The support vector classiﬁer, sometimes called a soft margin classiﬁer, does exactly this. Rather than seeking the largest possible margin so that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observations to be on the incorrect side of the margin, or even the incorrect side of the hyperplane. (The margin is soft because it can be violated by some of the training observations.) An example is shown in the left hand panel of Figure 6. . . Most of the observations are on the correct side of the margin, but a small set of observations are on the wrong side of the margin(observation 1 and 8). An observation can not only be on the wrong side of margin but also on the wrong side of the hyperplane. In case no hyperplane exist then such a scenario is inevitable. Observations corresponds to the wrong side of the hyperplane are observations that are misclassified by support vector classifier. Observations which lie on correct side of the margin does not affect the support vector classifier, but the observations that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors. These observations do aﬀect the support vector classiﬁer. . The support vector classiﬁer is a natural approach for classiﬁcation in the two-class setting, if the boundary between the two classes is linear. However, in practice we are sometimes faced with non-linear class boundaries. For instance consider the data shown in left panel of Figure 7. . . It is clear that a support vector classiﬁer or any linear classiﬁer will perform poorly here. Indeed, the support vector classiﬁer shown in the right-hand panel of Figure 7 is useless here. . We can address this problem by enlarging the feature space using quadratic, cubic or higher order polynomial functions of the features. The support vector machine (SVM) is an extension of the support vector classiﬁer that results from enlarging the feature space in a speciﬁc way, using kernels. For example, In Figure 7 we have two features X1 and X2, rather than using these features as it is for classification, we can also include higher degree terms of these features. Such as, X1² and X2² , which will give us a quadratic polynomial whose solution is nonlinear. We use kernels for doing exactly this in an efficient manner, where we specify what kind of decision boundary to use. For example: linear, polynomial (with some degree) and radial. Figure 8 illustrates the use of polynomial and radial kernel on the data of Figure 7. . . On the left hand panel we used a polynomial kernel with degree 3 and on the right we have used a radial kernel. Both the kernels resulted in more appropriate decision rule. The mathematics of how decision boundaries and kernels are obtained, are too technical to discuss here. . Reference: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie and Daniela Witten. .",
            "url": "https://clublinguistics.github.io/blogs/2020/09/09/svm.html",
            "relUrl": "/2020/09/09/svm.html",
            "date": " • Sep 9, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Tsne",
            "content": "What is t-SNE ? . t-SNE ( t-Distributed Stochastic Neighbor Embedding) is a technique that visualizes high dimensional data by giving each point a location in a two or three dimensional map. The technique is the variation of Stochastic Neighbor Embedding (SNE) that is much easier to optimize and produces significantly better visualization. . There can be several other techniques which can be used for visualizing high dimensional data, like: PCA, which is a linear technique that focuses on keeping the low dimensional representation of dissimilar data points far apart. For a high dimensional data that lies on or near a low dimensional nonlinear manifold, it is usually more important to keep the low dimensional representation of very similar data points as close as possible, which is typically not possible with the linear mapping. t-SNE is capable of capturing much of the local structure of the high dimensional data, while also revealing the global structure, such as, presence of cluster at several scales. Figure 1 shows use of t-SNE on MNIST dataset which consist of images of hand written digits. . . It starts by converting the high dimensional Euclidean distances between data points into joint probabilities that represent similarities. For example, Similarity of high dimensional data point xj to that of xi is the joint probability pji. pji can be interpreted as probability that xi would pick xj as its neighbor, if neighbor were picked in proportion to their probability density under Normal(Gaussian) distribution. For nearby point pji will be high whereas, for widely separated data points pji will be infinitesimal. Figure 2 represents how distances are mapped as probabilities on a Normal curve. . . Now for the low dimensional counterpart say, yi and yj of the high dimensional data points, xi and xj, it is possible to compute similar joint probability qji. Since, in high dimensional space we converted distances into probabilities using a Normal distribution, in the low dimensional map we can use a probability distribution that has a much heavier tail than Normal, to convert distances into probabilities. This allows a moderate distance in the high dimensional space to be faithfully modeled by a much larger distance in the map, and as a result it eliminates the unwanted attractive forces between map points, which represents moderately dissimilar data points (Crowding Problem). In t-SNE we employ a student t-distribution as a heavy tailed distribution in the low dimensional map. Figure 3 illustrates the differences between Normal distribution and t-distribution. . . If the map points yi and yj correctly model the similarity between high dimensional data points xi and xj, the joint probabilities pji= qji. Therefore, t-SNE aims to find a low dimensional representation that minimizes the mismatch between pij and qji. . In computation of t-SNE there is a parameter involved called ‘perplexity’ which can be interpreted as a smooth measure of effective number of neighbors, whose typical value is between 5 and 50. . Reference: Visualizing Data using t-SNE, research paper by Laurens van der Maaten and Geoffrey Hinton. .",
            "url": "https://clublinguistics.github.io/blogs/2020/07/20/tsne.html",
            "relUrl": "/2020/07/20/tsne.html",
            "date": " • Jul 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Gam",
            "content": "What are GAMs? . Generalized additive models (GAMs) provide a general framework for extending a standard linear model by allowing nonlinear functions of each of the variables, while maintaining additivity. Let’s see what exactly that means, . Linear models are simple to describe and implement and have advantage over other approaches in terms of interpretation and inference. But they have limitations in prediction power, that is, how accurately we can predict the output. Suppose we have data which consist of input of P features (X1, X2,….., Xp), and a output Y. Therefore, the corresponding linear model (also known as multi linear regression model) to predict the output: . Y = β0 + β1X1 + β2X2 +···+ βpXp + Ɛ . Where β0, β1,….,βp are parameters of the equation and Ɛ is the irreducible error , in order to allow for non-linear relationships between each feature and the response(output) is to replace each linear component βjXj with a (smooth) nonlinear function fj(Xj) which corresponds to the jth feature . We would then write the model as . Y = β0 + f1(X1) + f2(X2) + f3(X3) +…..+ fp(Xp)+Ɛ . This is an example of a GAM. It is called an additive model because we calculate a separate fj for each Xj, and then add together all of their contributions. Now the question is how to find this nonlinear function? It turns out there are various methods, but we will specifically be looking at Natural Splines for below example: . Wage = β0 + f1(year)+f2(age)+f3(education)+ Ɛ — — — — — -(1) . Before discussion on natural splines it is worth noting that the relationship which exist in real world data is often nonlinear, and a lot of time very complex, that is, even a standard nonlinear function will not prove to be a good approximation of the relation. Now, natural splines are piece-wise degree ‘d’ polynomials whose first ‘d-1’ derivatives are continuous with additional boundary constraints , Instead of ﬁtting a high-degree polynomial over the entire range of feature space, piece-wise polynomial regression involves ﬁtting separate low-degree polynomials, to be concrete, in the equation (1) we are predicting wage on the basis of years, age and education. Here we are independently fitting the functions keeping other features constant, that is, prediction of ‘wage’ on the basis of ‘age’ keeping ‘year’ and ‘education’ constant, Now we know as the ‘age’ increases ‘wages’ increases but after retirement the wages fall, that means up to a certain ‘age’ the relationship is increasing and after which it is decreasing therefore, we fit a polynomial until say age 60 which gives increasing relationship and then after 60, another polynomial to capture decreasing relationship, so it unable us to be flexibly extract relationship between feature and the response. The constraints(continuity of derivatives) unable us to smoothly join these two polynomials. . Now coming back to GAMs, here ‘year’ and ‘age’ are quantitative variables, and ‘education’ is a qualitative variable with ﬁve levels: &lt;HS, HS, &lt;Coll, Coll ,&gt;Coll, referring to the amount of high school or college education that an individual has completed. We ﬁt the ﬁrst two functions using natural splines. We ﬁt the third function using a separate constant for each level, via the dummy variable approach (for each level of education we create a separate feature with binary value 0 or 1, for example, in case person has high school (HS) as education, ‘HS’ will be 1 and for every other feature of levels it will be 0. ) . . Figure 1 shows the results of ﬁtting the model using least squares to predict wages on the basis of ‘years’ keeping age and education constant. Wage tends to increase slightly with year; this may be due to inﬂation. . . Figure 2 indicates that holding education and year ﬁxed, wage tends to be highest for intermediate values of age, and lowest for the very young and very old. . . Figure 3 indicates that holding year and age ﬁxed, wage tends to increase with education: the more educated a person is, the higher their salary, on average. . The main limitation of GAMs is that the model is restricted to be additive. With many variables, important interactions can be missed. . Reference: An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. .",
            "url": "https://clublinguistics.github.io/blogs/2020/06/25/gam.html",
            "relUrl": "/2020/06/25/gam.html",
            "date": " • Jun 25, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Pca",
            "content": "What is PCA ? . Principal component analysis (PCA) is to find the subset of variables that best explains the variation in the data. Let’s see what exactly that means, . It often happens there are so many features in the data set and small fraction of information is present in each feature or variable. For ex: suppose we have a dataset which consist of 50 columns (features), it will be almost impossible to visualize this 50 features in the same plot (like 50 dimensional view or 2 dimensional plot for every feature VS 49 others) and look for insights. So what we do is that we find a low-dimensional representation of the data that captures as much of the information as possible. If we can obtain a two-dimensional representation of the data that captures most of the information, then we can plot the observations in this low-dimensional space. PCA provides a tool to do just this. It finds a low-dimensional representation of a data set that contains as much as possible variation. The idea is that each of the say ’n’ observations that lives in some p-dimensional space, not all of these dimensions are equally interesting. PCA seeks a small number of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each dimension. Let’s look at an small example, suppose we have an advertisement data which consist of two features: population size (pop) in tens of thousands of people, and ad spending for a particular company (ad) in thousands of dollars, for 100 cities (no. of observations). So for the two features we will have a two dimensional view of the data. When we look for the lower dimensional view, we are set to find principal components using the data or given feature which best explains the variation in the data(which are typically less than the number of features present in the data) . . . The green solid line in Fig 1 represents the first principal component direction(Z1) of the data. We can see by eye that this is the direction along which there is the greatest variability in the data (In general more populated cities will have larger Ad Spending). That is, if we projected the 100 observations onto this line then the resulting projected observations would have the largest possible variance; projecting the observations onto any other line would yield projected observations with lower variance. Projecting a point onto a line simply involves finding the location on the line which is closest to the point, illustrated in fig2. . . Which means we are projecting the original data from 2D plane to 1D line Now in the fig1 there is also a blue dashed line which is the second principal component(Z2) and is perpendicular(orthogonal) to the first(green line). Projecting the points on to this line will explain lesser variance than the first. So there are two important properties of principal components: . 1) Principal component directions are orthogonal. . 2) The next principal component always explains lesser variance than the previous. . Now since the data itself is 2 dimensional we cannot have more than 2 principal component vector. In the figure1 having 3rd principal component will have the same direction as of 1st. Note: Population and Ad spending are measured on different scales ,that is for population measured in number of people and for Ad spending measured in dollars, Therefore we need to standardized them (removing the effect of measurement). Now what about the dataset for which features are greater than 2? We illustrate the use of PCA on the USArrests data set. For each of the 50 states (number of observations) in the United States, the data set contains the number of arrests per 100,000 residents for each of three crimes: Assault, Murder, and Rape. We also record UrbanPop (the percent of the population in each state living in urban areas). . PCA was performed after standardizing each variable to have mean zero and standard deviation one, It is necessary because measurement of UrbanPop and for that of crimes are in different scales .Having the mean zero enable us to shift the origin so that all the features are measured from the same point. . Figure 3 plots the first two principal components of these data. . . In the above figure value for the principal component (1 and 2) value less than 0 indicates below average (since values for all the features are standardized). For example, for the state Montana the value for Murder, Assault, Rape and Urbanpop are less than average. . Note: The Urbanpop, Murder, Assault and Rape are all features of the USArrest Dataset. . In Figure3, we see that the first loading vector(X-axis values for each feature ) places approximately equal weight on Assault, Murder, and Rape, with much less weight on UrbanPop. Hence this component roughly corresponds to a measure of overall rates of serious crimes. The second loading vector (Y-axis values for each feature) places most of its weight on UrbanPop and much less weight on the other three features. Hence, this component roughly corresponds to the level of urbanization (how much populated the state is) of the state. Overall, we see that the crime-related variables (Murder, Assault, and Rape) are located close to each other, and that the UrbanPop variable is far from the other three. This indicates that the crime-related variables are correlated with each other — states with high murder rates tend to have high assault and rape rates — and that the UrbanPop variable is less correlated with the other three. Our discussion of the loading vectors suggests that states with large positive scores on the first component, such as California, Nevada and Florida, have high crime rates, while states like North Dakota, with negative scores on the first component, have low crime rates. California also has a high score on the second component, indicating a high level of urbanization, while the opposite is true for states like Mississippi. States close to zero on both components, such as Indiana, have approximately average levels of both crime and urbanization. . PCA projects the data onto lower dimensions (which is within itself is so amazing!) which helps in visualization and drawing insights(Exploratory Data Analysis), There is one more application in which the principal components are used as features to predict the output(Supervised learning) known as Principal component regression(PCR). . Computation of principal component is too technical to discuss here, it involves the concept of linear algebra:Singular Value Decomposition(SVD). . Reference: An Introduction to Statistical Learning with Applications in R, book by Robert Tibshirani, Gareth James, Trevor Hastie, Daniela Witten. .",
            "url": "https://clublinguistics.github.io/blogs/2020/06/09/pca.html",
            "relUrl": "/2020/06/09/pca.html",
            "date": " • Jun 9, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Nn",
            "content": "What is a Neural Network ? . Neural Network is the means by which computer learns to perform some task with the help of data,which vaguely corresponds to how human brain works.Let’s see this in detail with an example. . Whenever a real estate broker wants to sell any house he will look for the best price with profit at which the house could be sold. The broker itself figures out the price of the house on the basis of some features like: Size, No. of bedrooms and locality, his experience will help in figuring out the optimal price.The question is: How neural network will do the same task? In human what we called ‘experience’ is analogous to data for the neural network through which neural network will learn to predict the house prices. Let’s assume for now we have only one feature for predicting the price of the house : Size, and generally the increase in the Size of the house is directly proportional to the increase in Price. i.e. As the Size of the house increases the Price also increases. . . From Fig 1. we can observe that Price of the house increases with increase in the Size. Since, neither Size nor Price of the house starts from zero, the origin corresponds to some positive value, instead of zero. So now what exactly the neuron is doing ? For every given input of ‘Size’ the neuron is trying to predict the ‘Price’ for the house(Fig 2) , So neuron is just computing the function in which ‘Price’ of the house is related to the ‘Size’ and in this case it is linear (from Fig 1). Now we are left with one question How the neural network figures out the function and it’s values? ( like in case of linear we have to find the slope and intercept of the line). Let’s see this in detail: . Now we are quite sure that only a single feature will not give the optimal prediction of the price that is,seeing only the size of the house we do not estimate the price for it. Therefore, Let’s assume the data consist of 3 input features: No. of bedrooms, Size, Locality and a single output that is, House price. So in training the neural network (for learning to predict the prices: it’s just like you are preparing for the exams using previous year question paper for which the solutions are known ) we will have input-output pairs i.e. for every single House with input of the features (Size, locality, No. of bedrooms) there will be a single output ‘House Price’ . that is, given the Size, locality, No. of bedrooms for a house as input and the known output(which is already present in the data) ‘price’ for that house. What is the Mathematical Function best suited for predicting the price as close as the observed price for that house ? That is, ‘predicted prices must be as close as the observed prices of the house’. This is exactly what we have to do, we have to minimize the difference between observed price and Predicted Price, and the steps involved is called the ‘Back Propagation steps’. And, finding the value of the price for input features of a house using the mathematical function the steps involved is known as ‘Forward Propagation Steps’. . What is the function of the neuron in the hidden layer? Every neuron in the hidden layer takes all the input features (Size, locality, No. of bedrooms) given in figure 3. But it’s not necessary they compute the same thing. So if we consider very 1st neuron, and let input from locality is 0 i.e. we will have only Size(It’s the size of the house) and No. of . Bedrooms for that house. It’s not hard to see that family size can be predicted from those two features, so even though family size is not given in the input features(Data) , it is predicted by the neuron in the hidden layer, similarly, other features can also be extracted. Similarly, you can easily see output neuron(neuron after the hidden layer) will predict the House price. So, it’s so far clear on high level what exactly neurons do, now what about hidden layers? So hidden layer helps in extracting features which are not explicitly provided like: ‘Family Size’, therefore, more the number of hidden layer more complex features to be extracted( like: Nose on the face in an image). And which will indeed help in more accurate predictions(extracting more complex features). . The complete process describes as follows: In training, the network starts with some assumption for the function and it’s values, to predict the house price. For each House It tries to predict the price(forward propagation) and sees how much error it makes (difference between original price and predicted) and try to minimize the error (backward Propagation). So, while predicting the next house price the function would’ve been slightly better since it is learning from each example , so more the data more it will learn, and will be accurate at predicting. The function after training is used for test were output(house prices) is not shown to the network (Just like after the preparation(training) you give the exams(testing)). In testing , trained network will be given inputs(size, locality and no. of bedrooms) from the test data and network predicts the House prices which are then compared with the original prices from the test data to see how well the network has performed ( Just like results of the exams ). Then the decision been made either to retrain the network with modifications(like if student failed in the exams) or to deploy in the real world . . Note: Here the test data is the unseen data which neural network have not seen during training. . Reference : Neural networks and deep learning Course by Andrew Ng, Coursera .",
            "url": "https://clublinguistics.github.io/blogs/2020/05/17/nn.html",
            "relUrl": "/2020/05/17/nn.html",
            "date": " • May 17, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://clublinguistics.github.io/blogs/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://clublinguistics.github.io/blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://clublinguistics.github.io/blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}